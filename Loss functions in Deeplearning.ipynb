{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98295a7f",
   "metadata": {},
   "source": [
    "<p>The function we want to minimize or maximize is called the objective function or criterion. When we are minimizing it, we may also call it the cost function, loss function, or error function.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd68d898",
   "metadata": {},
   "source": [
    "<h1> Mean Absolute Error (MAE)<h1>\n",
    "<img src=\"mae.png\">   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c511ec6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total error is 2.5\n",
      "MAE is 0.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "y_predicted = np.array([1,1,0,0,1])\n",
    "y_actual = np.array([0.30, 0.7, 1, 0, 0.5]) \n",
    "\n",
    "def mae(y_actual, y_predicted):\n",
    "    total_error = 0\n",
    "    for ya, yp in zip(y_actual, y_predicted):\n",
    "        total_error += abs(ya - yp)\n",
    "    print(\"total error is\", total_error)\n",
    "    print(\"MAE is\", total_error/len(y_predicted))\n",
    "    return total_error/len(y_predicted)\n",
    "mae(y_actual, y_predicted)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "045589dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# short eqn using np\n",
    "np.mean(abs(y_actual - y_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bbe6e1",
   "metadata": {},
   "source": [
    "<h1> Mean Squared Error </h1>\n",
    "<img src=\"mse.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3adabd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total error is 1.83\n",
      "MSE is 0.366\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.366"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "y_predicted = np.array([1,1,0,0,1])\n",
    "y_actual = np.array([0.30, 0.7, 1, 0, 0.5]) \n",
    "\n",
    "def mse(y_actual, y_predicted):\n",
    "    total_error = 0\n",
    "    for ya, yp in zip(y_actual, y_predicted):\n",
    "        total_error += (ya - yp) ** 2\n",
    "    print(\"total error is\", total_error)\n",
    "    print(\"MSE is\", total_error/len(y_predicted))\n",
    "    return total_error/len(y_predicted)\n",
    "mse(y_actual, y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c443758",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.366"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# short eqn using np\n",
    "np.mean((y_actual - y_predicted) ** 2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b5742c",
   "metadata": {},
   "source": [
    "<h1> Binary Crossentropy </h1>\n",
    "<p> BCE loss is used for the binary classification tasks. If you are using BCE loss function, you just need one output node to classify the data into two classes. The output value should be passed through a sigmoid activation function and the range of output is (0 – 1).</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e54d77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1e-15\n",
    "def log_loss(y_true, y_predicted):\n",
    "    y_predicted_new = [max(i,epsilon) for i in y_predicted]\n",
    "    y_predicted_new = [min(i,1-epsilon) for i in y_predicted_new]\n",
    "    y_predicted_new = np.array(y_predicted_new)\n",
    "    return -np.mean(y_true*np.log(y_predicted_new)+(1-y_true)*np.log(1-y_predicted_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30566be7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.2696280766844"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss(y_actual, y_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66806b4",
   "metadata": {},
   "source": [
    "<h1>Categorical Crossentropy</h1>\n",
    "<p> When we have a multi-class classification task, one of the loss function you can go ahead is this one. If you are using CCE loss function, there must be the same number of output nodes as the classes. And the final layer output should be passed through a softmax activation so that each node output a probability value between (0–1).</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc2c974b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5018b0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
